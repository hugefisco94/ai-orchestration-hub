{"id":"gen-1772025594-gXe0HOek714Nbju7FgE5","created":1772025595,"model":"swarm/deepseek-v3","object":"chat.completion","choices":[{"finish_reason":"stop","index":0,"message":{"content":"```json\n{\n  \"hero_title\": \"Military-Grade AI Orchestration Command Center\",\n  \"hero_subtitle\": \"Deploy trusted multi-agent systems in DDIL environments with built-in verification.\",\n  \"section_models\": \"Test AI models under realistic pressure and network degradation.\",\n  \"section_agents\": \"Coordinate specialized tool-using agents for planning and intelligence fusion.\",\n  \"section_architecture\": \"Build sovereign AI systems with on-premise deployment and deterministic reasoning.\",\n  \"section_cloud\": \"Extend cloud-independent AI capabilities to the tactical edge for resilience.\"\n}\n```","role":"assistant","provider_specific_fields":{"reasoning":null,"refusal":null},"annotations":[{"type":"url_citation","url_citation":{"end_index":0,"start_index":0,"title":"Orchestrate - Rotascale","url":"https://rotascale.com/products/orchestrate/","content":"Orchestrate - Rotascale Orchestrate | Rotascale\n\nOrchestrate\n\n# Multi-Agent Platform\n\nDeploy reliable agent systems with built-in verification. Visual builder, audit logs, and enterprise-grade access control.\n\n[Request a demo] [View pricing] Download PDF\n\nEnterprise-grade orchestration for multi-agent systems you can trust\n\nInteractive Demo\n\n### Experience ARTEMIS in action\n\nWatch N-agent debates unfold, explore hierarchical reasoning, and see real-time safety monitoring.\n\n[Try the Demo →] \n\nThe problem\n\n## Multi-agent systems are hard to trust\n\nAgent systems make decisions autonomously. When those decisions matter-in finance, healthcare, or operations-you need verification, auditability, and control.\n\n#### Coordination complexity\n\nMultiple agents need to work together without conflicts or failures. Emergent behavior can be unpredictable and hard to debug.\n\n#### Verification gaps\n\nHow do you know agents are making correct decisions? Standard testing doesn't work for autonomous systems.\n\n#### Auditability requirements\n\nRegulators and stakeholders need to understand agent reasoning. Black-box decisions don't fly in regulated industries.\n\n#### Access control\n\nEnterprise deployments need fine-grained permissions. Not every agent should have access to every resource.\n\n\"Debate protocols and consensus mechanisms validate agent outputs-before they reach production.\"\n\nSee It In Action\n\n## Trust Cascade\n\nDifferent decisions require different verification. Click each scenario to see how Orchestrate routes decisions.\n\nLow Stakes Answer FAQ Medium Stakes Update record High Stakes Process refund Critical Override policy\n\nRequest\n\nAgent\n\nAnalyzing...\n\nVerify\n\nPeer review\n\nDebate\n\nConsensus\n\nHuman\n\nApproval\n\nExecute\n\nLogged\n\nAudit Log LIVE\n\nArchitecture Patterns\n\n## Agent Topologies\n\nDifferent workflows need different agent arrangements. Click to explore each topology.\n\nHierarchical Peer-to-Peer Hybrid\n\nManager Orchestrator\n\nAgent A Research\n\nAgent B Analysis\n\nAgent C Action\n\n#### Hierarchical\n\nManager agent delegates tasks to specialized workers. Clear chain of command. Best for structured workflows with defined steps.\n\nAgent A Equal\n\nAgent B Equal\n\nAgent C Equal\n\nAgent D Equal\n\n#### Peer-to-Peer\n\nAgents collaborate as equals, sharing context directly. No single point of control. Best for debate protocols and consensus decisions.\n\nCoordinator Routing\n\nTeam Alpha\n\nA1\n\nA2\n\nTeam Beta\n\nB1\n\nB2\n\n#### Hybrid\n\nCoordinator routes to specialized teams. Teams collaborate internally as peers. Best for complex workflows with multiple domains.\n\nCapabilities\n\n## Enterprise-grade agent orchestration\n\nBuild, deploy, and manage reliable multi-agent systems with built-in verification and auditability.\n\n### Visual agent builder\n\nDesign agent workflows with a drag-and-drop interface. Define agent roles, handoffs, and verification checkpoints visually.\n\n### Built-in verification\n\nDebate protocols and consensus mechanisms validate agent outputs. Multiple agents verify decisions before execution.\n\n### Reasoning capture\n\nFull audit logs of agent reasoning chains. Every decision is documented with the reasoning that led to it.\n\n### Trust cascade\n\nRoute decisions through appropriate verification levels. High-stakes decisions get more scrutiny automatically.\n\n### Human-in-the-loop\n\nConfigurable checkpoints for human review and approval. Define when humans need to intervene.\n\n### Role-based access control\n\nEnterprise-grade RBAC for agent systems. Control which agents can access which resources and actions.\n\nHow it works\n\n## From design to deployment\n\nOrchestrate gives you the tools to build agent systems that enterprises can actually trust.\n\n01\n\n#### Design\n\nBuild agent workflows in the visual builder. Define roles, responsibilities, and handoff conditions.\n\n02\n\n#### Configure\n\nSet verification rules, human-in-the-loop checkpoints, and access controls.\n\n03\n\n#### Deploy\n\nRun agents in production with monitoring and logging. Guardian integration for reliability monitoring.\n\n04\n\n#### Audit\n\nReview reasoning chains and generate compliance reports. Full traceability for every decision.\n\n2026 Architecture\n\n## Multi-agent governance at scale\n\n81% of organizations plan to expand into complex multi-agent systems in 2026. Orchestrate provides the governance layer that makes this safe.\n\n### Agent topology management\n\nSupport for hierarchical, peer-to-peer, and hybrid agent topologies. Define reporting structures, delegation chains, and escalation paths. Visualize agent relationships and dependencies.\n\n### Shared context governance\n\nWhen agents share memory and context, governance gets complex. Orchestrate controls what context flows between agents, with audit trails for every context exchange.\n\n### Consensus mechanisms\n\nCritical decisions can require agreement between multiple agents. Built-in voting, debate protocols, and consensus requirements configurable by decision type.\n\n### Coordination primitives\n\nSemaphores, locks, and barriers for agent coordination. Prevent race conditions when multiple agents act on shared resources. Deadlock detection and resolution.\n\n### Emergent behavior monitoring\n\nMulti-agent systems exhibit emergent behaviors that no single agent intended. Orchestrate detects unexpected patterns and alerts before they become problems.\n\n### Cost attribution across agents\n\nTrack LLM costs, compute costs, and tool costs by agent, team, and workflow. Understand the true cost of complex multi-agent operations.\n\nOpen source\n\n### Built on rotalabs-cascade\n\nOrchestrate is built on our open-source multi-agent frameworks: rotalabs-cascade and ARTEMIS. Verify our verification methods.\n\n[View on GitHub →] \n\nPricing\n\n## Plans for every scale\n\n#### Starter\n\n$1,000/month\n\n5 agents, 100K runs, visual builder, basic logging. For teams getting started with agents.\n\n#### Pro\n\n$5,000/month\n\n25 agents, 1M runs, full audit logs, RBAC. For production agent systems.\n\n#### Enterprise\n\nCustom\n\nUnlimited agents, on-premise deployment, custom SLA, dedicated support.\n\n[View full pricing details →] \n\nGet started\n\n## See Orchestrate in action\n\nSchedule a personalized demo with our team.\n\n[Request a demo]"}},{"type":"url_citation","url_citation":{"end_index":0,"start_index":0,"title":"From Chaos to Control: Why Orchestration Is the Real Future of AI in C2","url":"https://www.legionintel.com/blog/from-chaos-to-control-why-orchestration-is-the-real-future-of-ai-in-c2","content":"AI Orchestration in Command and Control\n\n7 minute read\n\n[Defense] \n\n# From Chaos to Control: Why Orchestration Is the Real Future of AI in C2\n\nPublished on\n\nDec 10, 2025\n\n[Legion] \n\nSecure AI\n\nLegion’s AI summary\n\nMost AI breaks in command and control because systems can't communicate—revealing the gap between pilot demos and operational reality. Here's what you'll learn:\n\n- Why governance must be architectural: As AI moves from generating answers to executing workflows, attribution, audit logs, and granular permissions become foundational requirements, not policy add-ons\n- How to build trust through field testing: Why realistic exercises under pressure reveal failure modes that lab testing can't detect, and how to ensure AI works when networks degrade\n- Why orchestration is the breakthrough: Cross-system coordination enables AI to complete work automatically, not just provide insights\n- Why AI pilots fail in C2: Most organizations test models, not architecture—missing the critical gap of how AI works across disconnected systems\n- [Get a Demo] \n\nAI has proven itself in intelligence analysis, autonomous systems, and knowledge management. But real-world command and control—where minutes or lives are at stake—exposes a critical gap: Most AI breaks the moment systems degrade or disconnect.\n\nThat reality defined this year's Air Force Association panel with leaders from Raft, Legion, Oracle, and Virtualitics. Colonel Ryan \"Ape\" Hayde, commander of the 505th Command and Control Wing, opened with the challenge every C2 leader faces: AI must be tested and trusted in realistic exercises long before it reaches a mission.\n\n\"We always try to practice like we fight. AI can't just happen in the fight. It can't be the thing we only unleash when it's time to go.\"\n\n> [Watch the full panel discussion] \n\nThe panel revealed a simple truth: AI's promise and its operational reality in C2 are still far apart. Most defense organizations deploy AI as isolated tools; think chatbots, computer vision models, and NLP pipelines. But command and control isn't a collection of point solutions. It's a system of systems. And when those systems can't coordinate, AI fails.\n\nThe breakthrough isn't better models. It's solving how AI works across every system.\n\n## Why AI Pilot Programs Fail\n\nOrganizations test whether models can generate good answers, rather than testing whether AI can actually complete work across their systems. That's because certain barriers make piloting AI across systems difficult, in both military and commercial operations:\n\n- The last-mile problem: AI works in demos but breaks under real complexity\n- Security requirements that eliminate most commercial solutions\n- Data locked in legacy formats\n- Siloed systems that don't communicate\n\nThese aren't technical challenges—they're architectural ones.\n\nPeter Guerra, Group Vice President of AI & ML at Oracle Government, Defense & Intelligence, described what production actually looks like: Oracle processes 85% of its purchase orders autonomously, moving \"from knowledge to action.\"\n\nThat's the goal: getting from pilot to production. And it isn't about better models—it's about solving how AI connects across systems.\n\n## Why Orchestration Is The Answer\n\nTrey Coleman, Chief Product Officer at Raft, framed the current state of AI within C2 bluntly: \"Most of how we're using AI today is autonomy, is if-then statements.\"\n\nIn other words, AI is being used to automate simple or isolated tasks. But complex operations, like those within C2, require something fundamentally different.\n\nConsider what Col. Hayde described: You plan 100 aircraft to launch. Seventy actually launch. Fifty return. Meanwhile, three runways are destroyed, assets have moved, and aircraft need coordinated landing instructions across multiple bases—right now.\n\nIn this moment, humans shouldn't be manually:\n\n- Coordinating landing instructions across bases\n- Tracking asset locations in real-time\n- Calculating fuel availability and routes for returning aircraft\n- Checking runway statuses across systems\n\nAs Coleman put it, \"What workload can I offload to a machine so I only have to think about the hard things?\" In this case, the “hard thing” is the critical decision of how to safely recover the mission. The above scenario proves that the bottleneck isn't human analysis. It's friction between systems that don't communicate.\n\nBen Van Roo, CEO of Legion, explained where AI is headed: \"Artificial intelligence is getting more proficient in orchestrating all these different systems together—giving you the best picture given the information available.\"\n\nThis isn't theoretical. Legion's work with U.S. Special Operations Command demonstrates what this looks like in production:\n\n- 216 hours per week reclaimed from manual workflows\n- 24X faster surveillance log analysis\n- 94% faster intelligence summary generation\n\nThese are production results from AI that connects to existing systems, treating models as interchangeable components rather than dependencies.\n\n## Why Trust Requires Testing Under Pressure\n\nThe orchestration architecture is necessary but not sufficient. Col. Hayde posed the harder question: How do you ensure operators trust AI outputs when lives are on the line? The answer is in testing out in the field.\n\n\"As soon as you start getting real reps in exercises, you can see where we can trust it and where things fall over,” Van Roo explained.\n\nField testing reveals failure modes that are impossible to detect in laboratories. In one Joint Readiness Training Center rotation, Legion's team discovered a workflow that broke when systems desynced—a problem that only emerged under realistic operational conditions.\n\nGuerra added the technical reality that makes testing essential: \"These are statistical systems with a degree of error—10, 15, 20%. [They] require testing to make sure you're okay with those margins.\"\n\nTrust isn't built through technical specifications or vendor promises. It requires:\n\n- Complete attribution of every action taken\n- Repeatable outcomes across multiple scenarios\n- Extensive practice in realistic conditions\n\n## When Everything Breaks: AI in Degraded Environments\n\nMost commercial AI platforms are built on assumptions of constant connectivity, high bandwidth, and centralized infrastructure. When communications are contested and the cloud is unavailable, they simply stop working.\n\nThe military equivalents are obvious: contested networks, jammed communications, tactical edge operations. The critical question is then: Does your AI work when conditions degrade? And if it breaks, what do you do then? It's an issue on leaders' minds.\n\n\"I'm always worrying that we might really start trusting all of this stuff, and then if we don't have that connection—what's our decision cycle look like?\" Van Roo said.\n\nOne way to protect against this is by using an AI platform like Legion, which is built for operations (military or commercial) where favorable conditions cannot be assumed and does not require the cloud to function.\n\nOther key foundational features that will stop your AI from failing include:\n\n- Deployment options: Supporting cloud, on-prem, air-gapped, and edge\n- Data sovereignty: Processing within your environment, not external clouds\n- Model flexibility: Treating models as interchangeable components\n\n## Building Governance Into the Architecture\n\nAs soon as AI moves beyond just generating answers and starts executing across platforms, governance becomes an architectural requirement, not a policy decision.\n\nColeman posed the ethical question underlying all AI deployment: \"We could build a machine to make really complex decisions. We can build anything, but should we? I personally draw the line at decisions about taking human life.\"\n\nIt's the right line to draw. But it highlights a broader governance challenge.\n\n### Why Procedural Governance Isn't Enough\n\nAs AI completes more work automatically across operational systems, how do you maintain accountability? The answer cannot be procedural, with policies and review processes bolted onto systems after deployment. Instead, governance must be architectural:\n\n- Intelligent observability: Providing visibility into resource usage and performance\n- Granular permissions: Governing access at the workflow and model level\n- Audit logs: Capturing every action in real-time\n- Attribution: Showing not just what was done, but on whose behalf\n\nThe question shifts from \"Did a human approve this?\" to \"Is every action traceable, auditable, and operating within defined policies—even when executed by autonomous agents?\"\n\n## Moving Forward\n\nVan Roo's closing was direct: \"It's gonna move as fast as we want it to.\"\n\nAI in command and control is moving beyond chatbots. The future is AI that executes over multiple platforms—systems coordinating across applications, completing work automatically, and operating under human-defined policies, even when networks are contested.\n\nThe next decade of advantage won't go to those who adopt AI first, but rather those who operationalize it best.\n\nReady to move past demos? Learn how Legion helps enterprises and government agencies deploy AI with security, flexibility, and control built in. [Contact our team].\n\n‍\n\n## Frequently asked questions\n\n[Back to Blogs] \n\n## Table of Contents\n\n- {{ Template Heading for Styling purposes }}\n\nHow AI orchestration transforms C2 across every system. Learn why cross-system execution, not autonomous AI, ensures reliable operations."}},{"type":"url_citation","url_citation":{"end_index":0,"start_index":0,"title":"The Sovereign AI Architecture Guide v2.0 | ArcaQ","url":"https://www.arcaq.com/en/blog/sovereign-ai-architecture-guide-v2.html","content":"The Sovereign AI Architecture Guide v2.0 | ArcaQ\n\n[Back to Resources] \n\n## Executive Summary\n\nThis guide presents production-validated architectural patterns for building Sovereign AI systems through multi-agent orchestration. Drawing from production-validated implementations in regulated industries, this document provides a blueprint for building enterprise AI systems that prioritize data sovereignty, deterministic reasoning, and multi-jurisdictional compliance.\n\n#### What Makes AI \"Sovereign\"?\n\nSovereign AI inverts the cloud model: instead of renting probabilistic inference from external providers, organizations own deterministic intelligence on their own hardware. It's built on five foundational pillars:\n\n##### Data Sovereignty\n\n100% on-premise deployment with zero external dependencies\n\n##### Computational Sovereignty\n\nCPU-optimized inference without GPU vendor lock-in\n\n##### Regulatory Sovereignty\n\nMulti-jurisdictional compliance automation\n\n##### Knowledge Sovereignty\n\nCertified knowledge graphs over probabilistic generation\n\n##### Operational Sovereignty\n\nExpert-in-the-loop validation and continuous improvement\n\n## Part I: Foundations of Sovereign AI\n\n### The Crisis of Cloud-Dependent AI\n\nModern enterprise AI systems face a fundamental sovereignty crisis. Organizations have become critically dependent on external cloud services, creating three critical vulnerabilities:\n\n1. Regulatory Vulnerability - Cloud AI services often violate data sovereignty laws in regulated industries\n2. Economic Vulnerability - Unpredictable costs that can spike 10x during usage surges\n3. Strategic Vulnerability - Loss of control over AI capabilities during geopolitical tensions or vendor disputes\n\n> \"The future of enterprise AI lies not in more powerful cloud models, but in systems that organizations can truly own, control, and evolve independently of external vendors.\"\n\n### Key Architectural Principles\n\n#### Principle 1: Separation of Concerns\n\nTraditional monolithic AI systems fail at sovereign intelligence because they conflate concerns. Multi-agent architectures provide separation of concerns through autonomous, specialized agents that communicate via standardized protocols.\n\n- Independent scaling and fault isolation\n- Technology heterogeneity is supported (Python, Java, Go interoperability)\n- Agents communicate via message passing (not direct coupling)\n- Each agent handles a single responsibility\n\n## Part II: Multi-Agent Architecture Principles\n\n### The Agent Model\n\nAn agent in a sovereign AI architecture is defined as:\n\n> A software component with autonomous decision-making capability, communicating with other agents via asynchronous message passing, maintaining its own state, and providing a well-defined service interface.\n\n#### Key Properties of Agents\n\n- Social Ability - Communicates with other agents via protocols\n- Proactivity - Can initiate actions to achieve goals\n- Reactivity - Responds to events/messages in environment\n- Autonomy - Agent decides when and how to execute tasks\n\n### Agent Communication Patterns\n\nEffective multi-agent systems require standardized communication protocols. Industry best practices include:\n\n- Circuit Breaker Patterns - Graceful degradation when agents are unavailable\n- Service Mesh Integration - Secure mTLS communication between agents\n- Event-Driven Architecture - Agents react to domain events asynchronously\n- Message-Based Communication - Agents exchange JSON-based messages via message queues\n\n## Part III: Data Sovereignty Implementation\n\n### The Zero External Dependency Principle\n\nTrue data sovereignty requires that all data processing occurs 100% on-premise with zero external API calls. This includes:\n\n- Inference Layer - LLM execution happens on local CPUs/GPUs\n- Storage Layer - Knowledge graphs and embeddings reside on controlled infrastructure\n- Processing Layer - All transformations, enrichment, and analysis occur locally\n- Ingestion Layer - Data never leaves organizational boundaries during extraction\n\n#### Implementation Guidelines\n\n- Implement data residency checks at the infrastructure level\n- Maintain local copies of all ML models and embeddings\n- Deploy container registries within the airgap\n- Use local certificate authorities for mTLS\n- Implement network policies that block all egress traffic by default\n\n### Cross-Border Data Transfer Controls\n\nWhen operations span multiple jurisdictions, implement jurisdictional arbitration to determine the strictest common compliance level:\n\n1. Log the arbitration decision for audit trails\n2. Validate that the operation satisfies all constraints\n3. Compute the intersection of all constraints (strictest wins)\n4. Load compliance rules for each jurisdiction from a dynamic registry\n5. Identify all jurisdictions involved in the data flow\n\n## Part IV: Multi-Jurisdictional Compliance\n\n### Dynamic Compliance Architecture\n\nModern enterprises operate across 60+ jurisdictions with constantly evolving regulations. Static, hardcoded compliance rules create maintenance nightmares. The solution: dynamic compliance packs stored in databases, not code.\n\n#### Compliance Pack Structure\n\nEach jurisdiction should have a modular compliance pack defining:\n\n- Encryption Standards - Minimum encryption algorithms required\n- Audit Requirements - What must be logged for compliance\n- Breach Notification - Timeline for security incident reporting\n- Right to Erasure - Implementation of data deletion rights\n- Retention Policies - How long data must be kept\n- Consent Requirements - What user permissions are needed\n- Data Residency Rules - Where data must physically reside\n\n#### Best Practice: Hot-Reloadable Compliance\n\nStore compliance rules in a database with versioning support. When regulations change:\n\n1. Previous rule versions retained for audit trail\n2. No application redeployment required\n3. Compliance engine automatically loads new rules at midnight\n4. Insert new rule version with effective date\n\n### PII Detection and Anonymization\n\nBefore any data reaches an LLM, it must pass through PII detection. Industry best practices include:\n\n- Log all PII detections for compliance reporting\n- Maintain entity mapping for reversible anonymization when authorized\n- Implement multiple anonymization strategies (redaction, masking, hashing, tokenization)\n- Support 20+ languages and 50+ entity types\n- Use pre-trained NER models for entity recognition\n\n## Part V: Knowledge Graph Architecture\n\n### Deterministic Reasoning Over Probabilistic Generation\n\nThe key innovation in sovereign AI is prioritizing deterministic knowledge retrieval over probabilistic content generation. This approach eliminates the hallucination problem that plagues traditional RAG systems.\n\n#### The Two-Space Model\n\nSeparate your knowledge representation into two distinct spaces:\n\n##### Tensor Space\n\nTechnology: RDF Knowledge Graphs (Apache Jena, GraphDB)\n\nQuery Language: SPARQL (deterministic logic)\n\nConfidence: = 1.0 (certified facts only)\n\nUse Case: Factual queries with absolute certainty required\n\n##### Vector Space\n\nTechnology: Vector Databases (Qdrant, Weaviate, pgvector)\n\nQuery Language: Similarity search\n\nConfidence: 0.0\n\nUse Case: Semantic search, recommendations, fuzzy matching\n\n### Query Routing Strategy\n\nImplement intelligent query routing:\n\nRoute Accordingly\n\n- Hybrid queries → Query both, merge results with confidence scores\n- Semantic queries → Vector Space (similarity search)\n- Factual queries → Tensor Space (SPARQL)\n\n> \"It is better to admit ignorance than to hallucinate facts. In regulated industries, false negatives (saying 'I don't know') are acceptable, but false positives (stating incorrect facts) can have legal and financial consequences.\"\n\n## Part VI: On-Premise LLM Strategy\n\n### CPU-First Inference\n\nModern CPUs with specialized instructions (Intel AMX, AMD AVX-512) can achieve 2-4x inference speedup compared to standard FP32, making on-premise LLM deployment economically viable.\n\n#### Cost Analysis Framework\n\nWhen evaluating CPU vs GPU for on-premise LLM:\n\n- Operational Complexity - CPUs: Standard ops | GPUs: Specialized CUDA/driver management\n- Deployment Flexibility - CPUs: Available in all data centers | GPUs: Limited availability\n- Cooling Requirements - CPUs: Standard air cooling | GPUs: Specialized cooling infrastructure\n- Power Consumption - CPUs: 200-300W | GPUs: 400-700W\n- Hardware Costs - High-end server CPUs: $5-10K | Enterprise GPUs: $30-100K\n- Handle Knowledge Gaps - If no certified facts exist, admit ignorance rather than generate\n\n#### When to Choose CPU Inference\n\n- Data center GPU availability limited\n- Cost optimization prioritized over raw throughput\n- Batch size = 1 (single user queries)\n- Latency requirements 1 second acceptable\n- Models under 13B parameters\n\n### Model Selection Criteria\n\nFor sovereign AI deployments, prioritize:\n\n1. Compact Size - 7-13B parameter models offer best cost/performance for CPU\n2. Fine-Tuning Friendly - Models that can be adapted to domain-specific terminology\n3. Multilingual Capability - Support for languages in your jurisdictions\n4. Quantization Support - Models that perform well in INT8/BF16 precision\n5. Open-Source Licensing - Avoid models with restrictive commercial licenses\n\n## Part VII: Security & Access Control\n\n### Relationship-Based Access Control (ReBAC)\n\nTraditional Role-Based Access Control (RBAC) fails in multi-tenant, hierarchical organizations. ReBAC provides fine-grained permissions based on relationships between users, resources, and organizations.\n\n#### ReBAC Implementation Patterns\n\n- Audit All Decisions - Log permission checks for compliance reporting\n- Check Permissions at Query Time - Every data access validates permission via ReBAC\n- Synchronize with IDP - Import organizational hierarchy from Azure AD, Okta, or Keycloak\n- Define Permission Model - Specify types (user, team, organization) and relations (member, owner, viewer)\n- Choose a ReBAC Engine - OpenFGA (Google Zanzibar-inspired), SpiceDB, or Ory Keto\n\n### Zero-Trust Architecture\n\nImplement zero-trust principles across your sovereign AI platform:\n\n- Continuous Verification - Re-authenticate and re-authorize on every request\n- Least Privilege - Agents have minimal permissions required for their function\n- Network Segmentation - Isolate agent workloads in separate network zones\n- Service Mesh - Deploy Istio or Linkerd for transparent mTLS and observability\n- Mutual TLS (mTLS) - All inter-agent communication uses certificate-based authentication\n\n## Part VIII: Enterprise Deployment Patterns\n\n### Kubernetes-Native Architecture\n\nDeploy sovereign AI systems on Kubernetes for portability, scalability, and operational excellence:\n\n#### Deployment Best Practices\n\n- Horizontal Scaling - Use HPA (Horizontal Pod Autoscaler) for demand-based scaling\n- Health Checks - Implement liveness and readiness probes for all agents\n- Resource Limits - Define CPU/memory limits for predictable performance\n- GitOps - Use ArgoCD or Flux for declarative, version-controlled deployments\n- Helm Charts - Package all agents as Helm charts with configurable values\n\n### Observability Stack\n\nComprehensive observability is critical for production sovereign AI:\n\n- Alerts - Alertmanager for proactive incident detection\n- Dashboards - Grafana for unified observability dashboards\n- Traces - Jaeger or Tempo for distributed tracing across agents\n- Logs - Loki or Elasticsearch for centralized log aggregation\n- Metrics - Prometheus for time-series metrics (request rates, latencies, error rates)\n\n#### Key Metrics to Monitor\n\n- Cache hit rates\n- PII detection accuracy\n- Compliance validation latency\n- LLM inference throughput (tokens/second)\n- Knowledge graph query performance\n- Agent availability and error rates\n- Query latency (p50, p95, p99)\n\n### Disaster Recovery & High Availability\n\nEnsure business continuity through:\n\n1. Regular DR Drills - Test recovery procedures quarterly\n2. Stateless Agents - Design agents to be stateless for easy failover\n3. Knowledge Graph Backup - Daily incremental backups of RDF store\n4. Database Replication - PostgreSQL streaming replication for metadata\n5. Multi-Zone Deployment - Distribute agents across availability zones\n\n## Conclusion: The Path Forward\n\nSovereign AI represents the next evolution in enterprise intelligence systems. By combining multi-agent orchestration, knowledge graphs, on-premise deployment, and dynamic compliance, organizations can build AI systems that they truly own and control.\n\nThe key principles to remember:\n\n- Expert-in-the-Loop - Humans validate, machines execute\n- Dynamic Compliance - Make regulations data, not code\n- Agent Autonomy - Build specialized agents with clear responsibilities\n- Determinism Over Probability - Prefer certified knowledge to generative guessing\n- Sovereignty First - Never compromise on data residency and control\n\n> \"The future belongs to organizations that view AI not as a cloud service to consume, but as a sovereign capability to cultivate.\"\n\n### Want the Complete Guide?\n\nGet the full 45-page PDF with detailed diagrams, architecture patterns, and implementation checklists.\n\n[Request Full Guide] \n\nAvailable for enterprise customers and partners\n\n### Accessibility Tools\n\nIncrease Text Decrease Text Grayscale High Contrast Reset All"}},{"type":"url_citation","url_citation":{"end_index":0,"start_index":0,"title":"Agentic AI for DDIL Environments | Centurion by Legion","url":"https://www.legionintel.com/whitepapers/centurion-the-legion-intelligence-platform-deployed-to-the-edge","content":"Agentic AI for DDIL Environments | Centurion by Legion\n\n# Centurion: The Legion Intelligence Platform, Deployed to the Edge\n\nPublished on\n\nJanuary 28, 2026\n\n[Download Whitepaper] \n\n## Executive Summary\n\nCenturion is Legion Intelligence’s agentic AI delivered as an integrated edge deployment. Centurion can run disconnected from the internet and remain operational in denied, degraded, intermittent, and limited (DDIL) environments. Centurion extends Legion’s core capabilities - agentic workflow orchestration, governed tool execution, sensitive data and system of record integration, auditability, and rapid model updates - into portable, resilient devices that can operate at the tactical edge and integrate across multiple edge nodes when networking is available.\n\nThis form factor augments tactical operations centers and command posts, operations and intelligence fusion cells, and activities in sensitive or denied environments to provide persistent cloud-independent AI capabilities where work actually happens. The Department of War’s recently released “AI-first” strategy memo explicitly sets expectations that agentic systems are first-class, model parity is mandatory, and compute resources must be available from datacenters to the edge. Centurion is a direct implementation of that posture: AI workflows that keep running when the network fails, and that can synchronize with cloud and enterprise environments when conditions allow.\n\nCenturion also fits squarely within the Department’s first Critical Technology Area - Applied Artificial Intelligence (AAI) - because it operationalizes AI for decision superiority by embedding AI into workflows and command-and-control contexts, including at the edge.\n\nLegion Intelligence supports national security users across USSOCOM, the U.S. Army, the U.S. Air Force, and DOE in secure AI-enabled workflows. In operational evaluations, Legion reduces end-to-end cycle time for Situation Report (SITREP) processing by 88% and Intelligence Summary (INTSUM) creation by 99% compared to baseline manual workflows. Legion has accelerated target system analysis by 15x and audio analysis by 10x. Together, these metrics reflect performance improvements across planning, intel analysis, and current-operations execution. Centurion extends these gains to DDIL environments with multiple configurations tailored to where warfighters operate.\n\n## Centurion Is Legion—Packaged for DDIL Reality\n\nThe Legion Intelligence platform is an agentic workflow orchestration layer: it connects authoritative data sources, coordinates tool-using agents, governs execution, and produces auditable outcomes that reduce operational cycle time and cognitive load. Centurion is Legion deployed as a portable, edge-native product line, built for environments where the internet is unavailable, bandwidth is constrained, or transmitting is operationally risky.\n\nCenturion is an operational AI system that governs agent behavior, coordinates workflows across humans and machines, and preserves continuity across edge and higher-echelon environments. It is not a collection of on-device agents or a point solution for inference at the edge.\n\nCenturion’s distinguishing attributes come from where it runs and how it survives constraints:\n\n- Cloud as accelerator, not dependency: Centurion is designed to extend Legion deployments downward (cloud → on-prem → edge) while preserving mission function when disconnected—explicitly rejecting “cloud-only” fragility.\n- Federated mesh operations: Centurion nodes are designed to operate as a federated mesh, coordinating agent execution, sharing context, and redistributing workloads when connectivity allows. This enables distributed operations across forward nodes without reliance on centralized infrastructure, preserving tempo and resilience even as network conditions fluctuate.\n- Local execution: agents, workflows, tools, and models run on the edge node so the workflow does not collapse when cloud access is lost.\n- [Get a Demo] \n\n## The Threat Environment Makes Connectivity a Liability\n\nThe modern battlefield produces more information than commanders and staff can process; the limiting factor is human attention and decision speed. Across domains, these teams are drowning in sensor feeds, operational messages, intelligence reporting, logistics signals, and public data—often arriving faster than a staff can triage, correlate, and brief. Meanwhile, peer adversaries deliberately contest the assumptions of “perfect connectivity” through jamming, cyber operations, physical attacks on infrastructure, emissions tracking, and deception. In this environment, architectures that quietly assume persistent cloud access externalize risk onto the warfighter: they work in permissive conditions and fail at the moment bandwidth is constrained, links are denied, or transmitting becomes operationally risky.\n\nThe core operational claim is simple: if AI cannot function when communications are disconnected, degraded, intermittent, or denied, it is not a warfighting capability. It becomes a peacetime analytics convenience—useful for back-office workflows, but unreliable for maneuver, sustainment, and decision-making under threat. DDIL is not an edge case; it is the operating condition that determines whether advanced automation helps the force or creates another fragile dependency.\n\nCenturion is built on that premise. It pushes agentic workflows to where decisions happen—forward C2 nodes, expeditionary maintenance and logistics elements, ISR cells, and tactical operations centers—so AI assistance does not disappear when the network does. Centurion’s edge form factor enables local execution of workflows and models, with governed tool use and auditable outputs, and can federate across forward nodes when connectivity permits. The result is resilient decision support that preserves tempo: local units can continue to analyze, plan, and act without waiting for reach-back.\n\nThis isn’t only about resilience; it’s also about velocity. In high-tempo operations, every additional tab, portal, manual correlation step, and copy/paste handoff consumes scarce cognitive bandwidth and slows the OODA loop. Agentic AI at the edge compresses complexity into fewer, higher-quality decision options grounded in authoritative data—surfacing what matters, when it matters—so humans spend their time deciding rather than searching. Just as importantly, by keeping processing local, the force reduces unnecessary data movement, lowers latency, and limits exposure that comes with constant “phone-home” dependencies, strengthening both operational security and continuity under pressure.\n\n## Aligned with the Department of War’s AI-First Strategy\n\nCenturion operationalizes the Department’s AI-first strategy memo: AI advantage is measured by deployment speed, DDIL survivability, and cycle-time reduction in real workflows, not polished demos in permissive environments. The memo prioritizes agentic systems, mandates rapid model parity, and directs compute expansion “from datacenters to the edge,” alongside federated data access and barrier removal to accelerate fielding. Centurion is designed to support mission command under contested conditions by preserving decision advantage when connectivity is denied. It enables decentralized execution by ensuring AI-enabled workflows remain available, governed, and auditable at the point of decision\n\n### Agentic systems are first-class requirements\n\nAgent Networks (AI-enabled battle management and decision support) and Enterprise Agents (rapid, secure deployment of agents to transform workflows) are two of the seven initial Pace-Setting Projects (PSPs). Centurion is Legion’s agentic runtime delivered to the edge, enabling both categories:\n\n- Enterprise-to-field continuity: enterprise workflow agents that keep working at forward nodes rather than stopping at the enclave boundary.\n- Operational agent networks: workflow agents that support planning-to-execution loops when comms are contested.\n\n### Compute expansion “from datacenters to the edge”\n\nThe Department will invest in compute infrastructure “from datacenters to the edge”. Centurion is the platform manifestation of that mandate: a repeatable, governed packaging of Legion for edge deployments that can operate fully offline and federate when connected.\n\n### AI model parity and rapid updates within 30 days\n\nWarfighters “cannot be working off models that are months or years old,” and directs a cadence enabling the latest models to be deployed within 30 days of public release as a primary procurement criterion. Centurion inherits Legion’s upgrade pathway and applies it to disconnected deployments: deploy updated models and agent policies to edge nodes with controlled rollout, validation, and rollback patterns—so the edge does not become a frozen, outdated AI island.\n\n## The Applied AI Critical Technology Area: Centurion as a Reference Implementation\n\nThe Department’s updated CTAs explicitly include Applied Artificial Intelligence (AAI) as a top priority. AAI is oriented toward transforming the Department into an “AI-first” organization and embedding AI into decision-making and operational efficiency. Centurion is a direct operationalization of AAI:\n\n- Centurion enables a datacenter-to-edge continuum, matching the memo’s compute directive and reducing reliance on fragile central architectures.\n- Centurion pushes AAI to the tactical edge, which is where decision superiority is contested.\n- Centurion embeds AI into real workflows (not standalone chat tools), with orchestration, governance, and auditability aligned to operational and enterprise use.\n\nCenturion can also support adjacent CTA outcomes (e.g., contested logistics decision cycles, and battlefield information dominance through faster sense/decide/act loops), but its primary positioning is straightforward: AAI delivered under wartime constraints.\n\n## Warfighter-Grade AI, Delivered Forward\n\nCenturion is Legion Intelligence’s agentic AI platform, hardened, packaged, and fielded in multiple edge form factors to deliver the full power of orchestrated AI workflows wherever the mission happens. It runs locally when disconnected, maintains governance and auditability in constrained conditions, and federates across forward nodes to preserve tempo without relying on fragile reach-back. With secure integration into sensitive data and systems of record, controlled model and agent updates, and a deployment footprint designed for tactical operations centers, command posts, and low-signature missions, Centurion is ready for the environments that break cloud-dependent tools.\n\nIn short, Centurion brings Legion’s proven orchestration and trust model to the edge—providing persistent, resilient AI assistance that remains available when bandwidth is limited, links are denied, and decisions cannot wait.\n\n‍\n\n[Back to Whitepapers] \n\n## Table of Contents\n\n- {{ Template Heading for Styling purposes }}"}},{"type":"url_citation","url_citation":{"end_index":0,"start_index":0,"title":"Orchestrating Intelligent Agents with Elsa Workflows","url":"https://sipkeschoorstra.medium.com/orchestrating-intelligent-agents-with-elsa-workflows-f3346b91dc17","content":"Orchestrating Intelligent Agents with Elsa Workflows | by Sipke Schoorstra | Medium\n[Open in app] \nSign up\n[Sign in] \n[] \n[\nWrite\n] \n[\n] \nSign up\n[Sign in] \n![] \n# Orchestrating Intelligent Agents with Elsa Workflows\n[\n![Sipke Schoorstra] \n] \n[Sipke Schoorstra] \nFollow\n16 min read\n·Sep 1, 2024\n[\n] \n--\n1\n[] \nListen\nShare\nOutsource Cognitive Tasks and Deploy a Team of Intelligent Agents to Supercharge Your Business Processes\n![] \n# Introduction\nA set of new modules just landed in the preview of Elsa 3.3 called**Agents**.\nWith Agents, you can define a set of personas that are specialized in a given cognitive task. These agents can then be added to a workflow from where they will perform their work as a background task and produce output that can then feed into subsequent activities —including other agents.\n> In this context, an agent refers to a preconfigured large language model (LLM) prompt, combined with execution settings and inputs that are provided to the LLM for processing.\nThe ability to outsource cognitive tasks to intelligent agents enables powerful use cases. For example, if you’re automating content publishing pipelines, you could deploy a Proofreader agent that checks articles for typos, grammar errors and spelling mistakes and fix them. Another agent might create some accompanying artwork, while another agent prepares meta description + keywords for SEO optimization and another agent prepares social media outings.\nThe potential is virtually limitless.\n# Setting Up Agents\nLet’s explore the Agents module in practice within the context of an actual Elsa Server application.\n> In case you don’t have an Elsa Server project setup yet, please follow [> this guide\n] > .\nSetting up your Elsa Server with support for Agents involves these main steps:\n1. Adding references to the relevant*Elsa.Agents.\\**packages.\n2. Configuring Agents in`appsettings.json`.\n3. Updating`Program.cs`.## Package References\nAdd the following packages to your Elsa Server project:\n```\ndotnet add package Elsa.Agents.Core --source https://f.feedz.io/elsa-workflows/elsa-3/nuget/index.json\ndotnet add package Elsa.Agents.Activities --source https://f.feedz.io/elsa-workflows/elsa-3/nuget/index.json\n```\nThese packages contain the core entities and services to define agents and interact with[Semantic Kernel].\nThe*Elsa.Agents.Activities*package dynamically generates activities based on agents defined in the application. This effectively enables you to execute agents from a workflow, as we’ll see in a moment.\n## Appsettings.json\nAlthough not the only way, an easy way to define agents is to leverage .NET’s configuration’s system and define agents from appsettings.json.\nWe’ll define a new parent section called “Agents”, which in turn contains 3 child sections:\n1. “ApiKeys”\n2. “Services”\n3. “Agents”\nThe following is a complete example of the “Agents” parent section in an`appsettings.json`file ((save the actual API key value, which you must obtain yourself and is best provided as an Environment variable):\n```\n{\n&quot;Agents&quot;: {\n&quot;ApiKeys&quot;: [\n{\n&quot;Name&quot;: &quot;MyApiKey&quot;,\n&quot;Value&quot;: &quot;my-api-key&quot;\n}\n],\n&quot;Services&quot;: [\n{\n&quot;Name&quot;: &quot;ChatCompletion&quot;,\n&quot;Type&quot;: &quot;OpenAIChatCompletion&quot;,\n&quot;Settings&quot;: {\n&quot;ModelId&quot;: &quot;gpt-4o&quot;,\n&quot;ApiKeyRef&quot;: &quot;MyApiKey&quot;\n}\n},\n{\n&quot;Name&quot;: &quot;TextToImage&quot;,\n&quot;Type&quot;: &quot;OpenAITextToImage&quot;,\n&quot;Settings&quot;: {\n&quot;ModelId&quot;: &quot;dall-e-3&quot;,\n&quot;ApiKeyRef&quot;: &quot;MyApiKey&quot;\n}\n}\n],\n&quot;Agents&quot;: [\n{\n&quot;Name&quot;: &quot;Copywriter&quot;,\n&quot;Description&quot;: &quot;A persona that writes articles on any topic.&quot;,\n&quot;Services&quot;: [\n&quot;ChatCompletion&quot;\n],\n&quot;FunctionName&quot;: &quot;WriteArticle&quot;,\n&quot;&quot;PromptTemplate&quot;&quot;: &quot;&quot;You are a writer of articles about any topic and any additional instructions given to you. The syntax of the article content will be markdown, and your response will be in JSON format containing a \\\\&quot;&quot;title\\\\&quot;&quot; field and a \\\\&quot;&quot;markdown\\\\&quot;&quot; field using the following JSON structure: { \\\\&quot;&quot;title\\\\&quot;&quot;: \\\\&quot;&quot;The title of the Article\\\\&quot;&quot;, \\\\&quot;&quot;markdown\\\\&quot;&quot;: \\\\&quot;&quot;The article content in markdown syntax.\\\\&quot;&quot; }. This is the topic: {{$Topic}}. The article consists of around {{$ParagraphCount}} paragraphs. The article must be written in the language {{$Language}} using the following style: {{$WritingStyle}}. Additional instructions (if any): {{$AdditionalInstructions}}.&quot;&quot;,\n&quot;InputVariables&quot;: [\n{\n&quot;Name&quot;: &quot;Topic&quot;,\n&quot;Description&quot;: &quot;The topic to write an article about.&quot;,\n&quot;Type&quot;: &quot;string&quot;\n},\n{\n&quot;Name&quot;: &quot;ParagraphCount&quot;,\n&quot;Description&quot;: &quot;The number of paragraphs to write.&quot;,\n&quot;Type&quot;: &quot;int&quot;\n},\n{\n&quot;Name&quot;: &quot;Language&quot;,\n&quot;Description&quot;: &quot;The language to write the text in.&quot;,\n&quot;Type&quot;: &quot;string&quot;\n},\n{\n&quot;Name&quot;: &quot;WritingStyle&quot;,\n&quot;Description&quot;: &quot;The style to write the text in.&quot;,\n&quot;Type&quot;: &quot;string&quot;\n},\n{\n&quot;Name&quot;: &quot;AdditionalInstructions&quot;,\n&quot;Description&quot;: &quot;Any additional instructions.&quot;,\n&quot;Type&quot;: &quot;string&quot;\n}\n],\n&quot;OutputVariable&quot;: {\n&quot;Name&quot;: &quot;Article&quot;,\n&quot;Description&quot;: &quot;The article consisting of a title and markdown text.&quot;\n},\n&quot;ExecutionSettings&quot;: {\n&quot;MaxTokens&quot;: null,\n&quot;Temperature&quot;: 0.5,\n&quot;TopP&quot;: 0.9,\n&quot;PresencePenalty&quot;: 0.5,\n&quot;FrequencyPenalty&quot;: 0.5,\n&quot;&quot;ResponseFormat&quot;&quot;: &quot;&quot;json\\_object&quot;&quot;\n}\n}\n]\n}\n}\n```\nIn the above example, we defined one agent called “Copywriter” and configured it to act as an agent that is skilled in writing articles in a given language on a given topic.\nNotice that we gave this agent the following prompt:\n```\nYou are a writer of articles about any topic and any additional instructions\ngiven to you.\nThe syntax of the article content will be markdown, and your response will\nbe in JSON format containing a &quot;title&quot; field and a &quot;markdown&quot; field using\nthe following JSON structure:\n{\n&quot;title&quot;: &quot;The title of the Article&quot;,\n&quot;markdown&quot;: &quot;The article content in markdown syntax.&quot;\n}.\nThis is the topic: {{$Topic}}.\nThe article consists of around {{$ParagraphCount}} paragraphs.\nThe article must be written in the language {{$Language}} using the\nfollowing style: {{$WritingStyle}}.\nAdditional instructions (if any): {{$AdditionalInstructions}}.\n```\nThe placeholders embedded in the double curly braces (`{{$SomePlaceholder}}`) will be substituted with actual values we provide via**input variables**.\nThe input variables are defined in the “InputVariables” array and contain metadata that will be leveraged by the*Elsa.Agents.Activities*module to generate activity input descriptors.\n## Program.cs\nUpdate your`Program.cs`file as follows:\n```\nbuilder.Services.AddElsa(elsa =&gt;\n{\n// Enabling the Agent Activities feature will automatically enable the core Agent feature, since that&#x27;s dependency feature.\nelsa.UseAgentActivities();\n});\n// Bind `AgentsOptions` against the &quot;Agents&quot; configuration section.\nbuilder.Services.Configure&lt;AgentsOptions&gt;(options =&gt;\n{\nbuilder.Configuration.GetSection(&quot;Agents&quot;).Bind(options);\n});\n```\n# Executing Agents from Workflows\nWith that in place, start your Elsa Server and Elsa Studio, and go ahead and create a new workflow. You will see a new Activity category called “Agents”:\n![] \nA new section “Agents” appears when there’s at least one Agent defined.\nNow, as you define additional agents in`appsettings.json`, additional activities will appear here.\nLet’s add an instance of the*Copywriter*activity to the workflow and configure it. For example:\n![] \nIn order to capture the output of the activity, define a Workflow Variable and bind it to the output.\n![] \nCreate a new variable called “Article” of type “object”.\n![] \nBind the Article variable to the Copywriter’s output.\nWith that in place, it’s time to hit that Play button:\n![] \nTry the workflow by clicking the Play button.\nAfter a few seconds, we’ll see the workflow completing:\n![] \nA job well done!\n# Elsa Studio + Agents UI\nAlthough defining agents using configuration might work well in some cases, it would be much more powerful if we could quickly define new agents on the fly without needing to restart our application. Additionally, future improvements might include test-driving agents right from the UI.\n> In case you don’t have an Elsa Studio project setup, please go ahead and follow [> this guide\n] > .\nTo do so, we need to do two things:\n1. Update Elsa Server with packages that provide storage for agent definitions and exposes a REST API.\n2. Update Elsa Studio with a package that installs the necesary UI components.## Update Elsa Server\nInstall the following packages:\n```\ndotnet add package Elsa.Agents.Api --source https://my.custom.feed/v3/index.json\ndotnet add package Elsa.Agents.Persistence.EntityFrameworkCore.Sqlite --source https://my.custom.feed/v3/index.json\n```\nUpdate`Program.cs`as follows:\n```\nelsa\n.UseAgentActivities()\n.UseAgentPersistence(persistence =&gt; persistence.UseEntityFrameworkCore(ef =&gt; ef.UseSqlite(sqliteConnectionString)))\n.UseAgentsApi();\n```\nRestart the application.\n## Update Elsa Studio\nAdd the following packages:\n```\ndotnet add package Elsa.Studio.Agents --source https://my.custom.feed/v3/index.json\n```\nUpdate`Program.cs`as follows:\n```\nbuilder.Services.AddAgentsModule(backendApiConfig);\n```\n> In case you’re wondering about that backendApiConfig variable, it is defined as follows:\n```\nvar backendApiConfig = new BackendApiConfig\n{\nConfigureBackendOptions = options =&gt; configuration.GetSection(&quot;Backend&quot;).Bind(options),\nConfigureHttpClientBuilder = options =&gt; options.AuthenticationHandler = typeof(AuthenticatingApiHttpMessageHandler),\n};\n```\nRestart the Elsa Studio project and refresh the browser. You should now see two new menu item sections:\n![] \nTwo new menu item sections appear: General/Agents and Settings/Agents.\nThe first things we should do now are:\n1. Register our API key.\n2. Define Services.## Register API Key\nTo register an API key, navigate to the API Keys page and click the*Create API Key*button.\nFrom there, simply enter a name and the actual API key.\n![] \nRegister your API Key.\nNext, define two new services:\n![] \nRegister two services: ChatCompletion and TextToImage.\nNotice that both services reference the API Key by name that we created in the previous step.\n# Orchestrating Agents\nNow that we have done our job, it is time to spawn a team of agents and put them to work.\nLet’s create the following agents:\n* Copywriter\n* SEO Specialist\n* Translator\n* Artist\n* Social Media Specialist\nEach of these are specialized personas that we will orchestrate in a workflow to perform the following tasks:\n* Write a piece of content on a given topic in a given language.\n* Extract keywords and generate a description for SEO purposes (think meta description + keywords).\n* Translate the article in two more languages.\n* Create artwork to accompany the article.\n* Write a catchy message that can be shared on all of the socials.## Creating Agents\nTo create a new agent via the UI, simply navigate to the*Agents*screen and create a new one. For each agent, we can specify many settings, which I will supply here in JSON format.\n> The module doesn’t currently support importing/exporting JSON, so unfortunately, you will have to copy and paste individual fields by hand for now.\n**Copywriter**\nThe Copywriter is responsible for writing articles. Here are its settings:\n```\n{\n&quot;Name&quot;: &quot;Copywriter&quot;,\n&quot;Description&quot;: &quot;A persona that writes articles on any topic.&quot;,\n&quot;Services&quot;: [\n&quot;ChatCompletion&quot;\n],\n&quot;FunctionName&quot;: &quot;WriteArticle&quot;,\n&quot;&quot;PromptTemplate&quot;&quot;: &quot;&quot;You are a writer of articles about any topic and any additional instructions given to you. The syntax of the article content will be markdown, and your response will be in JSON format containing a \\\\&quot;&quot;title\\\\&quot;&quot; field and a \\\\&quot;&quot;markdown\\\\&quot;&quot; field using the following JSON structure: { \\\\&quot;&quot;title\\\\&quot;&quot;: \\\\&quot;&quot;The title of the Article\\\\&quot;&quot;, \\\\&quot;&quot;markdown\\\\&quot;&quot;: \\\\&quot;&quot;The article content in markdown syntax.\\\\&quot;&quot; }. This is the topic: {{$Topic}}. The article consists of around {{$ParagraphCount}} paragraphs. The article must be written in the language {{$Language}} using the following style: {{$WritingStyle}}. Additional instructions (if any): {{$AdditionalInstructions}}.&quot;&quot;,\n&quot;InputVariables&quot;: [\n{\n&quot;Name&quot;: &quot;Topic&quot;,\n&quot;Description&quot;: &quot;The topic to write an article about.&quot;,\n&quot;Type&quot;: &quot;string&quot;\n},\n{\n&quot;Name&quot;: &quot;ParagraphCount&quot;,\n&quot;Description&quot;: &quot;The number of paragraphs to write.&quot;,\n&quot;Type&quot;: &quot;int&quot;\n},\n{\n&quot;Name&quot;: &quot;Language&quot;,\n&quot;Description&quot;: &quot;The language to write the text in.&quot;,\n&quot;Type&quot;: &quot;string&quot;\n},\n{\n&quot;Name&quot;: &quot;WritingStyle&quot;,\n&quot;Description&quot;: &quot;The style to write the text in.&quot;,\n&quot;Type&quot;: &quot;string&quot;\n},\n{\n&quot;Name&quot;: &quot;AdditionalInstructions&quot;,\n&quot;Description&quot;: &quot;Any additional instructions.&quot;,\n&quot;Type&quot;: &quot;string&quot;\n}\n],\n&quot;OutputVariable&quot;: {\n&quot;Name&quot;: &quot;Article&quot;,\n&quot;Description&quot;: &quot;The article consisting of a title and markdown text.&quot;\n},\n&quot;ExecutionSettings&quot;: {\n&quot;MaxTokens&quot;: null,\n&quot;Temperature&quot;: 0.5,\n&quot;TopP&quot;: 0.9,\n&quot;PresencePenalty&quot;: 0.7,\n&quot;FrequencyPenalty&quot;: 0.7,\n&quot;&quot;ResponseFormat&quot;&quot;: &quot;&quot;json\\_object&quot;&quot;\n}\n}\n```\n**SEO Specialist**\nThe SEO Specialist will do two things: extract keywords from a given text, and write a short description about the text.\n```\n{\n&quot;Name&quot;: &quot;SeoSpecialist&quot;,\n&quot;Description&quot;: &quot;A persona that extracts relevant keywords and creates a meta description, optimized for search engines.&quot;,\n&quot;Services&quot;: [\n&quot;ChatCompletion&quot;\n],\n&quot;FunctionName&quot;: &quot;ExtractKeywordsAndMetaDescription&quot;,\n&quot;&quot;PromptTemplate&quot;&quot;: &quot;&quot;You are an SEO Specialist. Extract relevant keywords and create a meta description that is optimized for search engines from the following text: {{$Content}}. Your response must be using the following JSON format: { \\\\&quot;&quot;keywords\\\\&quot;&quot;: [\\\\&quot;&quot;keyword1\\\\&quot;&quot;, \\\\&quot;&quot;keyword2\\\\&quot;&quot;], \\\\&quot;&quot;description\\\\&quot;&quot;: \\\\&quot;&quot;The meta description\\\\&quot;&quot; }.&quot;&quot;,\n&quot;InputVariables&quot;: [\n{\n&quot;Name&quot;: &quot;Content&quot;,\n&quot;Description&quot;: &quot;The content to extract from.&quot;,\n&quot;Type&quot;: &quot;string&quot;\n}\n],\n&quot;OutputVariable&quot;: {\n&quot;Name&quot;: &quot;SeoResult&quot;,\n&quot;Description&quot;: &quot;The description and keywords.&quot;,\n&quot;Type&quot;: &quot;object&quot;\n},\n&quot;ExecutionSettings&quot;: {\n&quot;Temperature&quot;: 0.5,\n&quot;TopP&quot;: 0.9,\n&quot;PresencePenalty&quot;: 0.5,\n&quot;FrequencyPenalty&quot;: 0.5\n}\n}\n```\n**Translator**\nThe Translator can translate any given text to any given language that the configured model supports. Given that our Copywriter produces a JSON object consisting of a Title and a Markdown field, we’ll instruct the Translator to take any JSON input and simply translate the contents of each field.\n```\n{\n&quot;Name&quot;: &quot;Translator&quot;,\n&quot;Description&quot;: &quot;A persona that translates text into a target language.&quot;,\n&quot;Services&quot;: [\n&quot;ChatCompletion&quot;\n],\n&quot;FunctionName&quot;: &quot;Translate&quot;,\n&quot;PromptTemplate&quot;: &quot;You are a Translator. Translate the provided text to the target language using the specified language or culture code. Do not change the content or meaning of the text. The text will be provided in the form of a JSON object, where each field contains text to be translated. Your response will be in the exact same JSON format, but with the original text values replaced with the translation for each field. The JSON containing the fields to translate: {{$Content}}. The target language or culture code: {{$Language}}&quot;,\n&quot;InputVariables&quot;: [\n{\n&quot;Name&quot;: &quot;Content&quot;,\n&quot;Description&quot;: &quot;The content to translate.&quot;,\n&quot;Type&quot;: &quot;string&quot;\n},\n{\n&quot;Name&quot;: &quot;Language&quot;,\n&quot;Description&quot;: &quot;The language or culture code to translate the content to.&quot;,\n&quot;Type&quot;: &quot;string&quot;\n}\n],\n&quot;OutputVariable&quot;: {\n&quot;Name&quot;: &quot;Content&quot;,\n&quot;Description&quot;: &quot;The translation result.&quot;,\n&quot;Type&quot;: &quot;object&quot;\n},\n&quot;ExecutionSettings&quot;: {\n&quot;MaxTokens&quot;: null,\n&quot;Temperature&quot;: 0.2,\n&quot;TopP&quot;: 0.9,\n&quot;PresencePenalty&quot;: 0.1,\n&quot;FrequencyPenalty&quot;: 0.1,\n&quot;&quot;ResponseFormat&quot;&quot;: &quot;&quot;json\\_object&quot;&quot;\n}\n}\n```\n**Artist**\nThe Artist can create corresponding artwork for any article.\n```\n{\n&quot;Name&quot;: &quot;Artist&quot;,\n&quot;Description&quot;: &quot;A persona that creates an image that visualizes the essence of a given text.&quot;,\n&quot;Services&quot;: [\n&quot;ChatCompletion&quot;,\n&quot;TextToImage&quot;\n],\n&quot;FunctionName&quot;: &quot;DesignGraphics&quot;,\n&quot;&quot;PromptTemplate&quot;&quot;: &quot;&quot;You are an Artist that generates an image by interpreting the essence of the following text: {{$Content}}. Your response will be JSON in the following format: {\\\\&quot;&quot;imageUrl\\\\&quot;&quot;: \\\\&quot;&quot;https://dalle.com/the-image.png\\\\&quot;&quot;}.&quot;&quot;,\n&quot;InputVariables&quot;: [\n{\n&quot;Name&quot;: &quot;Content&quot;,\n&quot;Description&quot;: &quot;The content to design graphics for.&quot;,\n&quot;Type&quot;: &quot;String&quot;\n}\n],\n&quot;OutputVariable&quot;: {\n&quot;Name&quot;: &quot;Image&quot;,\n&quot;Description&quot;: &quot;An URL to the created artwork.&quot;,\n&quot;Type&quot;: &quot;object&quot;\n},\n&quot;ExecutionSettings&quot;: {\n&quot;MaxTokens&quot;: null,\n&quot;Temperature&quot;: 1,\n&quot;TopP&quot;: 0.9,\n&quot;PresencePenalty&quot;: 0.0,\n&quot;FrequencyPenalty&quot;: 0.0,\n&quot;&quot;ResponseFormat&quot;&quot;: &quot;&quot;json\\_object&quot;&quot;\n},\n&quot;Plugins&quot;: [\n&quot;ImageGenerator&quot;\n]\n}\n```\n**Social Media Specialist**\nThe Social Media Specialist can write catchy posts for sharing on social media.\n```\n{\n&quot;Name&quot;: &quot;Social Media Specialist&quot;,\n&quot;Description&quot;: &quot;A persona that specializes in writing catchy social media posts based on a given piece of content.&quot;,\n&quot;Services&quot;: [\n&quot;ChatCompletion&quot;\n],\n&quot;FunctionName&quot;: &quot;WriteSocialMediaPost&quot;,\n&quot;&quot;PromptTemplate&quot;&quot;: &quot;&quot;You are a Social Media Specialist. Write a catchy social media post based on the following text: {{$Content}}. The post should be optimized for social media platforms such as X (formerly Twitter) , Facebook, LinkedIn, and Instagram. Your response must be in JSON format and contain a single field named &#x27;&#x27;post&#x27;&#x27; with the text of the social media post. Example: { \\\\&quot;&quot;post\\\\&quot;&quot;: \\\\&quot;&quot;Some catchy social media post.\\\\&quot;&quot; }&quot;&quot;,\n&quot;InputVariables&quot;: [\n{\n&quot;Name&quot;: &quot;Content&quot;,\n&quot;Description&quot;: &quot;The content to write a social media post for.&quot;,\n&quot;Type&quot;: &quot;string&quot;\n}\n],\n&quot;OutputVariable&quot;: {\n&quot;Name&quot;: &quot;Post&quot;,\n&quot;Description&quot;: &quot;The generated social media post.&quot;,\n&quot;Type&quot;: &quot;object&quot;\n},\n&quot;ExecutionSettings&quot;: {\n&quot;Temperature&quot;: 0.9,\n&quot;TopP&quot;: 0.9,\n&quot;PresencePenalty&quot;: 0.7,\n&quot;FrequencyPenalty&quot;: 0.7\n}\n}\n```\nThe final list should look like this:\n![] \nOur new army of content specialists.\n## Creating the Workflow\nNow it’s time to put our army of content specialist to work.\nCreate a new workflow called “Create and Publish Content” and go ahead and drag and drop our agents as you see fit. For example:\n![] \nA simple, yet powerful content processing workflow.\nAs you might have noticed, the majority of these agents will run in parallel. This is possible due to the fact that each agent activity’s`Kind`is configured to be a`Job`.\n> Elsa’s workflow engine offers a robust feature where activities designated as ‘Job’ are executed asynchronously as background jobs. This is particularly useful when you have multiple jobs queued up; they are processed concurrently, in parallel. This parallel execution capability ensures that tasks are handled efficiently, maximizing throughput and minimizing response time.\nRunning this workflow will produce content that is similar to the following:\nCopywriter:\n```\n{\n&quot;title&quot;:&quot;The Power of Microsoft Semantic Kernel&quot;,\n&quot;&quot;markdown&quot;&quot;:&quot;&quot;# The Power of Microsoft Semantic Kernel\\\\n\\\\n## Unleashing the Force of Data Understanding\\\\n\\\\nIn a galaxy not so far away, Microsoft has harnessed the power of the Semantic Kernel, a tool that brings unparalleled clarity to the chaotic universe of data. Like a Jedi wielding a lightsaber, this technology slices through ambiguity and illuminates the path to insight. With its ability to understand context and meaning, the Semantic Kernel transforms raw data into actionable intelligence.\\\\n\\\\n## The Architecture: A Masterpiece in Design\\\\n\\\\nCrafted with precision akin to a lightsaber\\\\u0027s hilt, the architecture of Microsoft\\\\u0027s Semantic Kernel is both elegant and formidable. It integrates seamlessly with existing systems, leveraging machine learning algorithms that are as precise as a Jedi\\\\u0027s mind trick. This architectural marvel ensures that businesses can tap into their data reserves with unprecedented ease and accuracy.\\\\n\\\\n## Real-World Applications: Harnessing Galactic Potential\\\\n\\\\nImagine navigating through vast amounts of unstructured data like a starship through hyperspace. The Semantic Kernel makes this possible by enabling real-time analysis and decision-making. From enhancing customer experiences to optimizing supply chains, its applications are as diverse as the stars in the sky. Businesses across sectors are already feeling the force of this revolutionary technology.\\\\n\\\\n## Empowering Users: The Jedi Council of Data Science\\\\n\\\\nMicrosoft\\\\u0027s vision extends beyond mere technology; it empowers users to become masters in their own right. With intuitive interfaces and robust support systems, even those new to data science can harness the full potential of the Semantic Kernel. It\\\\u0027s like being trained by Yoda himself—users gain wisdom and skill rapidly, transforming them into powerful guardians of their organization\\\\u0027s knowledge.\\\\n\\\\n## The Future: A New Hope for Data Management\\\\n\\\\nAs we look towards the horizon, it\\\\u0027s clear that Microsoft\\\\u0027s Semantic Kernel represents a new hope for data management. Its capabilities will continue to evolve, bringing even greater precision and understanding to complex datasets. In this ever-expanding universe of information, having such a powerful ally ensures that organizations remain ahead in their quest for knowledge and innovation.\\\\n&quot;&quot;\n}\n```\nSEO Specialist:\n```\n{\n&quot;keywords&quot;:[&quot;Microsoft Semantic Kernel&quot;,&quot;data understanding&quot;,&quot;machine learning&quot;,&quot;real-time analysis&quot;,&quot;data management&quot;],\n&quot;&quot;description&quot;&quot;:&quot;&quot;Discover the power of Microsoft\\\\u0027s Semantic Kernel, a revolutionary tool that transforms raw data into actionable intelligence with unparalleled clarity and precision. Learn how its advanced architecture and real-time analysis capabilities empower businesses to optimize operations and enhance customer experiences.&quot;&quot;\n}\n```\nSocial Media Specialist:\n```\n{\n&quot;&quot;post&quot;&quot;:&quot;&quot;\\\\uD83C\\\\uDF0C Unleash the power of Microsoft Semantic Kernel! \\\\uD83D\\\\uDE80 Like a Jedi\\\\u0027s lightsaber, it cuts through data chaos, turning raw info into actionable insights. With precision architecture and real-time analysis, it\\\\u0027s transforming businesses across galaxies. Ready to become a Data Science Jedi? #Microsoft #DataScience #Innovation&quot;&quot;\n}\n```\nTranslator (French):\n```\n{\n&quot;title&quot;:&quot;La Puissance du Noyau Sémantique de Microsoft&quot;,\n&quot;&quot;markdown&quot;&quot;:&quot;&quot;# La Puissance du Noyau Sémantique de Microsoft\\\\n\\\\n## Libérer la Force de la Compréhension des Données\\\\n\\\\nDans une galaxie pas si lointaine, Microsoft a exploité la puissance du Noyau Sémantique, un outil qui apporte une clarté inégalée àl\\\\u0027univers chaotique des données. Comme un Jedi maniant un sabre laser, cette technologie tranche àtravers l\\\\u0027ambiguïté et illumine le chemin vers la perspicacité. Avec sa capacité àcomprendre le contexte et le sens, le Noyau Sémantique transforme les données brutes en intelligence exploitable.\\\\n\\\\n## L\\\\u0027Architecture : Un Chef-d\\\\u0027œuvre de Conception\\\\n\\\\nConçu avec une précision semblable àcelle d\\\\u0027un manche de sabre laser, l\\\\u0027architecture du Noyau Sémantique de Microsoft est àla fois élégante et redoutable. Elle s\\\\u0027intègre parfaitement aux systèmes existants, tirant parti d\\\\u0027algorithmes d\\\\u0027apprentissage automatique aussi précis qu\\\\u0027un tour de passe-passe Jedi. Cette merveille architecturale garantit que les entreprises peuvent exploiter leurs réserves de données avec une facilité et une précision sans précédent.\\\\n\\\\n## Applications Réelles : Exploiter un Potentiel Galactique\\\\n\\\\nImaginez naviguer àtravers d\\\\u0027immenses quantités de données non structurées comme un vaisseau spatial dans l\\\\u0027hyperespace. Le Noyau Sémantique rend cela possible en permettant une analyse et une prise de décision en temps réel. De l\\\\u0027amélioration des expériences client àl\\\\u0027optimisation des chaînes d\\\\u0027approvisionnement, ses applications sont aussi diverses que les étoiles dans le ciel. Les entreprises de tous les secteurs ressentent déjà la force de cette technologie révolutionnaire.\\\\n\\\\n## Autonomiser les Utilisateurs : Le Conseil Jedi de la Science des Données\\\\n\\\\nLa vision de Microsoft va au-delà de la simple technologie ; elle permet aux utilisateurs de devenir maîtres en leur propre droit. Avec des interfaces intuitives et des systèmes de support robustes, même ceux qui sont nouveaux dans la science des données peuvent exploiter tout le potentiel du Noyau Sémantique. C\\\\u0027est comme être formé par Yoda lui-même—les utilisateurs acquièrent rapidement sagesse et compétence, se transformant en puissants gardiens du savoir de leur organisation.\\\\n\\\\n## L\\\\u0027Avenir : Un Nouvel Espoir pour la Gestion des Données\\\\n\\\\nEn regardant vers l\\\\u0027horizon, il est clair que le Noyau Sémantique de Microsoft représente un nouvel espoir pour la gestion des données. Ses capacités continueront d\\\\u0027évoluer, apportant encore plus de précision et compréhension aux ensembles complexes de données. Dans cet univers en constante expansion d\\\\u0027informations, avoir un allié aussi puissant garantit que les organisations restent en avance dans leur quête pour le savoir et l\\\\u0027innovation.\\\\n&quot;&quot;\n}\n```\nTranslator (Dutch):\n```\n{\n&quot;title&quot;:&quot;De Kracht van Microsoft Semantic Kernel&quot;,\n&quot;&quot;markdown&quot;&quot;:&quot;&quot;# De Kracht van Microsoft Semantic Kernel\\\\n\\\\n## De Kracht van Data Begrip Ontketenen\\\\n\\\\nIn een niet zo verre melkweg heeft Microsoft de kracht van de Semantic Kernel benut, een tool die ongeëvenaarde helderheid brengt in het chaotische universum van data. Net als een Jedi met een lichtzwaard snijdt deze technologie door ambiguïteit en verlicht het pad naar inzicht. Met zijn vermogen om context en betekenis te begrijpen, transformeert de Semantic Kernel ruwe data in bruikbare intelligentie.\\\\n\\\\n## De Architectuur: Een Meesterwerk in Ontwerp\\\\n\\\\nMet precisie vervaardigd zoals het handvat van een lichtzwaard, is de architectuur van Microsoft\\\\u0027s Semantic Kernel zowel elegant als formidabel. Het integreert naadloos met bestaande systemen en maakt gebruik van machine learning-algoritmen die net zo precies zijn als de gedachtenkracht van een Jedi. Dit architectonische wonder zorgt ervoor dat bedrijven hun datareserves met ongekende eenvoud en nauwkeurigheid kunnen benutten.\\\\n\\\\n## Toepassingen in de Praktijk: Galactisch Potentieel Benutten\\\\n\\\\nStel je voor dat je door enorme hoeveelheden ongestructureerde data navigeert als een ruimteschip door hyperspace. De Semantic Kernel maakt dit mogelijk door realtime analyse en besluitvorming mogelijk te maken. Van het verbeteren van klantervaringen tot het optimaliseren van toeleveringsketens, de toepassingen zijn net zo divers als de sterren aan de hemel. Bedrijven in verschillende sectoren voelen al de kracht van deze revolutionaire technologie.\\\\n\\\\n## Gebruikers Empoweren: De Jedi Raad van Datawetenschap\\\\n\\\\nMicrosoft\\\\u0027s visie gaat verder dan alleen technologie; het stelt gebruikers in staat om meesters op hun eigen gebied te worden. Met intuïtieve interfaces en robuuste ondersteuningssystemen kunnen zelfs nieuwkomers in datawetenschap het volledige potentieel van de Semantic Kernel benutten. Het is alsof ze getraind worden door Yoda zelf—gebruikers verwerven snel wijsheid en vaardigheden, waardoor ze krachtige bewakers worden van de kennis binnen hun organisatie.\\\\n\\\\n## De Toekomst: Een Nieuwe Hoop voor Databeheer\\\\n\\\\nAls we naar de horizon kijken, is het duidelijk dat Microsoft\\\\u0027s Semantic Kernel een nieuwe hoop vertegenwoordigt voor databeheer. Zijn capaciteiten zullen blijven evolueren, waardoor nog grotere precisie en begrip mogelijk wordt bij complexe datasets. In dit steeds uitdijende universum van informatie zorgt zo\\\\u0027n krachtige bondgenoot ervoor dat organisaties voorop blijven lopen in hun zoektocht naar kennis en innovatie.\\\\n&quot;&quot;\n}\n```\nArtist:\n![] \nGenerated by the Artist agent\n# Conclusion\nIn this article, I introduced the new Agents module for Elsa Workflows. We went over setting up the module in Elsa Server and Elsa Studio and then created a small set of agents and a workflow to exercise the agents.\nCombining workflows with AI offers a reliable, predictable and repeatable process in a powerful manner.\n## Resources\n* **Semantic Kernel:**[https://learn.microsoft.com/en-us/semantic-kernel/overview/] \n* **Elsa Source Code Repository:**[https://github.com/elsa-workflows/elsa-core] \n* **Elsa Documentation:**[https://v3.elsaworkflows.io] /\n* **Elsa Documentation (new version in progress):**[https://elsa-workflows.github.io/elsa-documentation/introduction.html] \n* **Workflow JSON:**[https://raw.githubusercontent.com/elsa-workflows/elsa-guides/main/src/assets/blog/orchestrating-Intelligent-agents-with-elsa-workflows/create-and-publish-content.json] \n[\nElsa Workflows\n] \n[\nSemantic Kernel\n] \n[\nLlm Agent\n] \n[\nAgentic Ai\n] \n[\nNet Core\n] \n[\n![Sipke Schoorstra] \n] \n[\n![Sipke Schoorstra] \n] \nFollow\n[## Written bySipke Schoorstra\n] \n[633 Followers] \n·[68 Following] \nSoftware Developer\nFollow\n## Responses (1)\n[] \nSee all responses\n[\nHelp\n] \n[\nStatus\n] \n[\nAbout\n] \n[\nCareers\n] \n[\nPress\n] \n[\nBlog\n] \n[\nPrivacy\n] \n[\nRules\n] \n[\nTerms\n] \n[\nText to speech\n]"}}]},"provider_specific_fields":{"native_finish_reason":"stop"}}],"usage":{"completion_tokens":121,"prompt_tokens":3261,"total_tokens":3382,"completion_tokens_details":{"audio_tokens":0,"reasoning_tokens":0},"prompt_tokens_details":{"audio_tokens":0,"cached_tokens":0},"cost":0.0221623,"is_byok":false,"cost_details":{"upstream_inference_cost":0.0021623,"upstream_inference_prompt_cost":0.0019566,"upstream_inference_completions_cost":0.0002057}},"provider":"Google"}